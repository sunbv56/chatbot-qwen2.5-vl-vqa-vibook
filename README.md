# Vibook VQA Chatbot (CPU-Only)

A Visual Question Answering (VQA) chatbot web application using FastAPI, Hugging Face Transformers, and a LoRA fine-tuned Qwen2.5-VL model. This project is designed **exclusively for CPU inference**â€”no GPU required. Users can upload book cover images and ask questions about them, receiving answers generated by the model.

> **Try the online demo:** [Hugging Face Space](https://huggingface.co/spaces/sunbv56/demo-qwen2.5-vl-vqa-vibook)

## Features
- Visual Question Answering (VQA) on book cover images
- FastAPI backend with HTML frontend
- **CPU-only**: No GPU required for inference
- Local LoRA fine-tuned model fallback (if Hugging Face loading fails)
- Example assets auto-downloaded on startup

## Project Structure
```
chatbot-flask-qwen2.5-vl-vqa-vibook/
â”œâ”€â”€ app.py                # Main FastAPI app
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ static/               # Static files (JS, CSS)
â”œâ”€â”€ templates/            # Jinja2 HTML templates
â”œâ”€â”€ assets/               # Example images (auto-downloaded)
â””â”€â”€ local-model/ (if error in load model from huggingface)
    â””â”€â”€ qwen2.5-vl-vqa-vibook/  # Model and processor files (my model fine tuned by LoRA)
```

## Setup Instructions (CPU-Only)

### 1. Clone the repository
```bash
git clone <repo-url>
cd chatbot-flask-qwen2.5-vl-vqa-vibook
```

### 2. Install dependencies
It is recommended to use a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Model Weights
- By default, the app tries to load the model from Hugging Face.
- If you encounter errors (e.g., no internet, rate limits), place your LoRA fine-tuned model files in `local-model/qwen2.5-vl-vqa-vibook/`.
- The local model is a LoRA fine-tuned version for book VQA tasks.

### 4. Run the application (CPU-only)
```bash
python app.py
```
- The server will start and print a local network URL (e.g., `http://192.168.x.x:8000`).
- Open this URL in your browser.

## Usage
- Upload a book cover image and enter a question in the web interface.
- The model will process the image and question, returning an answer.
- Example images are available in the `assets/` folder.

## API
### POST `/vqa`
- **Form fields:**
  - `question`: The question string
  - `image`: The image file (jpg/png)
- **Response:**
  - JSON with `question` and `answer` fields

## Model
- Uses [Qwen2.5-VL](https://huggingface.co/sunbv56/qwen2.5-vl-vqa-vibook) (or local LoRA fine-tuned copy)
- Runs **only on CPU** (float16)
- Model and processor loaded via Hugging Face Transformers
- Local fallback model is fine-tuned with LoRA for book VQA

## Notes
- On first run, example images are downloaded to `assets/`.
- If the model fails to load, check the model files and paths.
- For LAN access, the server binds to `0.0.0.0`.
- This project is for research and educational purposes only.

## Online Demo
You can try the VQA chatbot directly in your browser via Hugging Face Spaces:

ðŸ‘‰ [https://huggingface.co/spaces/sunbv56/demo-qwen2.5-vl-vqa-vibook](https://huggingface.co/spaces/sunbv56/demo-qwen2.5-vl-vqa-vibook)

---

## License
This project is for research and educational purposes. See model license for details. 